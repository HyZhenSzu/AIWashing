import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import List
import numpy

class LLMSCorer:
    def __init__(self, model_name="THUDM/chatglm3-6b-base"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).half().cuda()
        self.model.eval()
        self.device = next(self.model.parameters()).device

        # å‡†å¤‡è¯„åˆ†å€™é€‰ token IDs åˆ—è¡¨ï¼ˆ"0"â€“"9"ï¼‰
        self.rating_texts = [str(i) for i in range(10)]  # "0"â€“"9"
        self.rating_token_ids = []
        for t in self.rating_texts:
            token_ids = self.tokenizer.encode(t, add_special_tokens=False)
            # åŽ»æŽ‰å¼•å·æˆ–ç©ºæ ¼ç­‰æ— æ•ˆå­—ç¬¦
            while len(token_ids) > 0:
                first_str = self.tokenizer.decode([token_ids[0]])
                if first_str.strip("' ") == "":
                    token_ids = token_ids[1:]
                else:
                    break
            if len(token_ids) != 1:
                print(f"[Warning] Candidate '{t}' ä¸æ˜¯å• tokenï¼Œå°†è¢«å¿½ç•¥: {token_ids}")
                continue
            decoded = self.tokenizer.decode(token_ids)
            # print(f"[Init] Candidate '{t}' -> token_id: {token_ids[0]} -> decoded: '{decoded}'")
            self.rating_token_ids.append((int(t), token_ids[0]))

    def score_chunk(self, chunk: str) -> float:
        prompt = (
            "AI Washingæ˜¯æŒ‡å…¬å¸åœ¨å¸‚åœºè¥é”€ä¸­å¤¸å¤§æˆ–ç‚’ä½œAIç›¸å…³å†…å®¹ï¼Œå¸¸è§è¡¨çŽ°åŒ…æ‹¬ï¼š\n"
            "1. ä½¿ç”¨å¤§é‡AIç›¸å…³çš„æµè¡Œè¯ï¼ˆå¦‚æ™ºèƒ½ã€ç®—æ³•ã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰ï¼Œä½†ç¼ºä¹å®žé™…æŠ€æœ¯ç»†èŠ‚ã€‚\n"
            "2. è¿‡åº¦å®£ä¼ AIèƒ½åŠ›ï¼Œå®žé™…åº”ç”¨åœºæ™¯å’ŒæŠ€æœ¯å®žçŽ°ä¸æ˜Žç¡®ã€‚\n"
            "è¯·ç”¨æ•°å­—ï¼ˆ0~9ï¼‰ç»™ä¸‹é¢å†…å®¹çš„ AI Washing ç¨‹åº¦æ‰“åˆ†ï¼Œ0ä»£è¡¨æœ€ä½Žï¼Œ9ä»£è¡¨æœ€é«˜ã€‚\n"
            "ç›´æŽ¥è¾“å‡ºä»£è¡¨åˆ†æ•°çš„æ•°å­—ã€‚\n"
            "ç¤ºä¾‹ï¼š\n"
            "å†…å®¹ï¼šæˆ‘ä»¬å…¬å¸é‡‡ç”¨äº†æœ€å…ˆè¿›çš„AIæŠ€æœ¯ï¼Œæå‡äº†äº§å“æ™ºèƒ½åŒ–æ°´å¹³ã€‚\n"
            "è¯„åˆ†ï¼š8\n"
            "å†…å®¹ï¼šæˆ‘ä»¬äº§å“çš„AIæ¨¡å—åŸºäºŽæ·±åº¦å­¦ä¹ ï¼Œå…·ä½“é‡‡ç”¨äº†ResNetç»“æž„ï¼Œè®­ç»ƒé›†ä¸ºImageNetã€‚\n"
            "è¯„åˆ†ï¼š2\n"
            "çŽ°åœ¨è¯·å¯¹ä¸‹é¢å†…å®¹æ‰“åˆ†ï¼Œç›´æŽ¥è¾“å‡ºzeroåˆ°tençš„è¯„åˆ†ï¼š\n"
            f"{chunk}\nè¯„åˆ†ï¼š"
        )

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1,
                return_dict_in_generate=True,
                output_scores=True,
                do_sample=True,
                temperature=0.6,
                top_p=0.5,
                pad_token_id=self.tokenizer.eos_token_id
            )

        logits = outputs.scores[0][0]
        probs = F.softmax(logits, dim=-1)

        weight_sum = 0.0
        score_sum = 0.0
        for score, token_id in self.rating_token_ids:
            p = probs[token_id].item()
            weight_sum += p
            score_sum += score * p
        
        weighted_score = score_sum / weight_sum if weight_sum > 0 else 0.0

        # ðŸ”§ è¿™é‡Œæ–°å¢žè°ƒè¯•æ‰“å°ï¼šåŽŸå§‹ chunk + æœ€ç»ˆè¯„åˆ†
        print("\n=== DEBUG: Chunk & Weighted Score ===")
        print(f"Chunk Content:\n{chunk[:1000]}{'...' if len(chunk) > 1000 else ''}")  # ä»…æ˜¾ç¤ºå‰500å­—
        print(f"Weighted Score: {weighted_score:.3f}\n")

        return weighted_score

    def score_chunk_debug(self, chunk: str) -> float:
        """
        è°ƒè¯•ç‰ˆï¼šæ‰“å°å„å€™é€‰ï¼ˆ'0'~'10'ï¼‰çš„æ¦‚çŽ‡å¹¶è¿”å›žåŠ æƒå‡å€¼åˆ†æ•°ã€‚
        """
        prompt = (
            "AI Washingæ˜¯æŒ‡å…¬å¸åœ¨å¸‚åœºè¥é”€ä¸­å¤¸å¤§æˆ–ç‚’ä½œAIç›¸å…³å†…å®¹ï¼Œå¸¸è§è¡¨çŽ°åŒ…æ‹¬ï¼š\n"
            "1. ä½¿ç”¨å¤§é‡AIç›¸å…³çš„æµè¡Œè¯ï¼ˆå¦‚æ™ºèƒ½ã€ç®—æ³•ã€æ·±åº¦å­¦ä¹ ç­‰ï¼‰ï¼Œä½†ç¼ºä¹å®žé™…æŠ€æœ¯ç»†èŠ‚ã€‚\n"
            "2. è¿‡åº¦å®£ä¼ AIèƒ½åŠ›ï¼Œå®žé™…åº”ç”¨åœºæ™¯å’ŒæŠ€æœ¯å®žçŽ°ä¸æ˜Žç¡®ã€‚\n"
            "è¯·ç”¨æ•°å­—ï¼ˆ0~9ï¼‰ç»™ä¸‹é¢å†…å®¹çš„ AI Washing ç¨‹åº¦æ‰“åˆ†ï¼Œ0ä»£è¡¨æœ€ä½Žï¼Œ9ä»£è¡¨æœ€é«˜ã€‚\n"
            "ç›´æŽ¥è¾“å‡ºä»£è¡¨åˆ†æ•°çš„æ•°å­—ã€‚\n"
            "ç¤ºä¾‹ï¼š\n"
            "å†…å®¹ï¼šæˆ‘ä»¬å…¬å¸é‡‡ç”¨äº†æœ€å…ˆè¿›çš„AIæŠ€æœ¯ï¼Œæå‡äº†äº§å“æ™ºèƒ½åŒ–æ°´å¹³ã€‚\n"
            "è¯„åˆ†ï¼š8\n"
            "å†…å®¹ï¼šæˆ‘ä»¬äº§å“çš„AIæ¨¡å—åŸºäºŽæ·±åº¦å­¦ä¹ ï¼Œå…·ä½“é‡‡ç”¨äº†ResNetç»“æž„ï¼Œè®­ç»ƒé›†ä¸ºImageNetã€‚\n"
            "è¯„åˆ†ï¼š2\n"
            "çŽ°åœ¨è¯·å¯¹ä¸‹é¢å†…å®¹æ‰“åˆ†ï¼Œç›´æŽ¥è¾“å‡ºzeroåˆ°tençš„è¯„åˆ†ï¼š\n"
            f"{chunk}\nè¯„åˆ†ï¼š"
        )

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1,
                return_dict_in_generate=True,
                output_scores=True,
                do_sample=True,
                temperature=0.6,
                top_p=0.5,
                pad_token_id=self.tokenizer.eos_token_id
            )

        logits = outputs.scores[0][0]
        probs = F.softmax(logits, dim=-1)

        # æ‰“å° top-5 å€™é€‰ token
        topk = torch.topk(probs, 5)
        print("\n=== Top-5 Tokens ===")
        for rank, (idx, p) in enumerate(zip(topk.indices, topk.values)):
            decoded = self.tokenizer.decode([idx])
            print(f"{rank+1:2d}: token_id={idx.item():5d}, prob={p.item():.4f}, decoded='{decoded}'")

        weight_sum = 0.0
        score_sum = 0.0
        for score, token_id in self.rating_token_ids:
            p = probs[token_id].item()
            weight_sum += p
            score_sum += score * p
        
        return score_sum / weight_sum if weight_sum > 0 else 0.0
